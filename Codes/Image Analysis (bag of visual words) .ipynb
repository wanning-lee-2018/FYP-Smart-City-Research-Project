{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21906,"status":"ok","timestamp":1637589316682,"user":{"displayName":"LEE WAN NING _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11525349523156579986"},"user_tz":-480},"id":"8W-OCzHu-pA9","outputId":"5195b7b1-71cb-4400-b19e-d25ae7a85431"},"outputs":[],"source":["#connect your google drive to google colab for easier access under \"Files\" on the left sidebar\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkHWfzh51Zma"},"outputs":[],"source":["#import the necessary libraries\n","import cv2\n","import numpy as np\n","from skimage import io\n","import pandas as pd \n","from skimage.color import rgb2gray\n","from skimage.feature import ORB, match_descriptors\n","from sklearn.cluster import KMeans\n","from scipy.spatial import distance\n","import json\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wjv6xlLKw8X"},"outputs":[],"source":["#Extract image features from all images\n","descriptors_lst=[]\n","imgs_features=[]\n","keypoints_lst=[]\n","#reading data (from excel) using pandas\n","df=pd.read_excel(\"/content/drive/MyDrive/Qualtrics_Images_url.xlsx\",sheet_name=\"Qualtrics MRT images\",header=[0])\n","#iterating through each MRT image\n","for i, j in df.iterrows():\n","  img = io.imread('{}'.format(df.iloc[i][\"Image_url\"]))\n","  img = rgb2gray(img)\n","  #extract image features\n","  detector_extractor = ORB()\n","  detector_extractor.detect_and_extract(img)\n","  descriptors=detector_extractor.descriptors \n","  keypoints=detector_extractor.keypoints\n","  # descriptors_lst length= all the descriptors that can be identified in all the images, each as an element. \n","  descriptors_lst.extend(descriptors)\n","  # img_features length=220(220 images). within each element, there is a list of the descriptors belonging to each image\n","  imgs_features.append(descriptors)\n","  # keypoints_lst  length=220 (220 images). within each element, there is a list of the keypoints belonging to each image\n","  keypoints_lst.append(keypoints)\n","\n","#References\n","#https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNvrD_BTuX9k"},"outputs":[],"source":["#using k-means clustering to extract 500 visual words that are most representative from all MRT images (image features)\n","\n","# A k-means clustering algorithm who takes 2 parameter which is number \n","# of cluster(k) and the other is descriptors list(unordered 1d array)\n","# Returns an array that holds central points.\n","\n","def kmeans(k, descriptor_list):\n","    kmeans = KMeans(n_clusters = k, n_init=10)\n","    kmeans.fit(descriptor_list)\n","    visual_words = kmeans.cluster_centers_ \n","    return visual_words\n","    \n","# Takes the central points which is visual words    \n","visual_words = kmeans(500, descriptors_lst) \n","\n","\n","#References\n","#https://medium.com/@aybukeyalcinerr/bag-of-visual-words-bovw-db9500331b2f"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7B0TBwBV0py0"},"outputs":[],"source":["# computing the visual words histogram (For each MRT image (220), for each VW (500))\n","\n","\n","imgs_hist=[]\n","\n","# Find the index of the closest central point to the each orb descriptor. \n","# Takes 2 parameters the first one is an orb descriptor and the second one is the array of central points in k means\n","# Returns the index of the closest central point.  \n","def find_index(image, center):\n","    count = 0\n","    ind = 0\n","    for i in range(len(center)):\n","        if(i == 0):\n","           count = distance.euclidean(image, center[i]) \n","        else:\n","            dist = distance.euclidean(image, center[i]) \n","            if(dist < count):\n","                ind = i\n","                count = dist\n","    return ind\n","\n","for img in imgs_features:\n","  histogram = np.zeros(len(visual_words))\n","  for each_feature in img:\n","    ind = find_index(each_feature, visual_words)\n","    histogram[ind] += 1\n","  imgs_hist.append(histogram)\n","\n","  #References\n","#https://github.com/AybukeYALCINER/gabor_sift_bovw/blob/master/assignment1.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDMt1nhWbxUa"},"outputs":[],"source":["#saving the 500 visual words histogram to an excel sheet\n","name_lst=[]\n","for x in range(500):\n","  name_lst.append(\"VW\"+str(x))\n","\n","df2 = pd.DataFrame.from_records(imgs_hist, columns=name_lst)\n","with open(\"/content/drive/MyDrive/imgBOVW.xlsx\",'wb') as f:\n","  df2.to_excel(f, sheet_name=\"500VW\")\n","\n","  #References\n","#https://datascience.stackexchange.com/questions/26333/convert-a-list-of-lists-into-a-pandas-dataframe\n","#https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmtRSSfTeEYk"},"outputs":[],"source":["# running correlation on the BOVW histogram \n","\n","#reading data (from excel) using excel\n","df=pd.read_excel(\"/content/drive/MyDrive/imgBOVW.xlsx\",sheet_name=\"4corr\",header=[0],index_col=[0])\n","#using pearson's correlation\n","df2=df.corr(method ='pearson')\n","#export the correlation result to excel sheet\n","with pd.ExcelWriter('/content/drive/MyDrive/imgBOVW.xlsx',mode='a') as writer:  \n","  df2.to_excel(writer, sheet_name='BOVW_corr')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfvzLyGS_0iF"},"outputs":[],"source":["# computing the image features keypoints histogram (For each MRT image (220), for each VW (500))\n","\n","keypoints_hist=[]\n","# Find the index of the closest central point to the each orb descriptor. \n","# Takes 2 parameters the first one is an orb descriptor and the second one is the array of central points in k means\n","# Returns the index of the closest central point.  \n","def find_index(image, center):\n","    count = 0\n","    ind = 0\n","    for i in range(len(center)):\n","        if(i == 0):\n","           count = distance.euclidean(image, center[i]) \n","           #count = L1_dist(image, center[i])\n","        else:\n","            dist = distance.euclidean(image, center[i]) \n","            #dist = L1_dist(image, center[i])\n","            if(dist < count):\n","                ind = i\n","                count = dist\n","    return ind\n","\n","default_value = 0\n","lst=[i for i in range(500)]\n","keypoints_bin = dict.fromkeys(lst,default_value)\n","\n","\n","for img_idx in range(len(imgs_features)):\n","  img=imgs_features[img_idx]\n","  for each_feature in range(len(img)):\n","    ind = find_index(img[each_feature], visual_words)\n","\n","    ky_lst=keypoints_lst[img_idx][each_feature].tolist()\n","    if keypoints_bin[ind] == 0:\n","      keypoints_bin[ind]=[ky_lst]\n","    else:\n","      keypoints_bin[ind].append(ky_lst)\n","  keypoints_hist.append(keypoints_bin)\n","  keypoints_bin=keypoints_bin.fromkeys(keypoints_bin, 0)\n","\n","  #References\n","#https://github.com/AybukeYALCINER/gabor_sift_bovw/blob/master/assignment1.py\n","    #https://www.journaldev.com/32797/python-convert-numpy-array-to-list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7mtG3pCLnin"},"outputs":[],"source":["#saving the image features keypoints histogram to an excel sheet\n","\n","name_lst=[]\n","for x in range(500):\n","  name_lst.append(\"VW\"+str(x))\n","df2 = pd.DataFrame.from_dict(keypoints_hist, orient='columns')\n","with pd.ExcelWriter('/content/drive/MyDrive/imgBOVW.xlsx',mode='a') as writer:  \n","  df2.to_excel(writer, sheet_name='keypoints_500VW')\n","\n","  #References\n","#https://datascience.stackexchange.com/questions/26333/convert-a-list-of-lists-into-a-pandas-dataframe\n","#https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfhK_A2fpBQx"},"outputs":[],"source":["#  Identify visual words (for each image quality of Beautiful,Safe and Welcome) on each MRT images\n","\n","#function for drawing the visual words identified in the MRT images \n","def draw_keypoints(img_name,img_url, keypoints,metrics, color = (0, 255, 255)):\n","  for kp in keypoints:\n","    x, y = kp\n","    cv2.circle(img_url, (int(x), int(y)), 2, color)\n","\n","    #Note because i used skimage to load image, which will be loaded as RGB. \n","    #but since im uisng opencv functions on this image, they assume that the image is loaded as BGR\n","    #so i will need to convert the image to BGR such that opencv wont apply function on the image as if its BGR\n","    cv2.imwrite(\"/content/drive/MyDrive/BOVW_images2/{}/{}\".format(metrics,img_name),cv2.cvtColor(img_url,cv2.COLOR_RGB2BGR))\n","\n","\n","img_metrics=[\"Beautiful\",\"Safe\",\"Welcome\"]\n","name_lst=[]\n","for x in range(500):\n","  name_lst.append(\"VW\"+str(x))\n","\n","#reading data (from excel) using pandas\n","df2=pd.read_excel(\"/content/drive/MyDriveimgBOVW.xlsx\",sheet_name=\"keypoints_500VW\",usecols=\"B:SG\",header=[0])\n","df2.columns=name_lst\n","df3=pd.read_excel(\"/content/drive/MyDrive/Qualtrics_Images_url.xlsx\",sheet_name=\"Qualtrics MRT images\",header=[0])  \n","\n","#drawing visual words identified in each MRT image for each image quality (Beautiful, Safe, Welcome)\n","for i in img_metrics:\n","  df=pd.read_excel(\"/content/drive/MyDrive/imgBOVW.xlsx\",sheet_name=\"{}\".format(i), usecols=\"B:DL\",header=[0])\n","  for x,vws in df2[df.columns].iterrows():\n","    keypoints2_lst=[]\n","    for vw in vws:\n","      if type(vw)!=int:\n","        keypoints2_lst.extend(json.loads(vw))\n","    draw_keypoints(\"{}\".format(df3.iloc[x][\"Image_name\"]), io.imread('{}'.format(df3.iloc[x][\"Image_url\"])),keypoints2_lst,i) \n","\n","#References\n","#https://stackoverflow.com/questions/39316447/opencv-giving-wrong-color-to-colored-images-on-loading\n","#https://stackoverflow.com/questions/42406338/why-cv2-imwrite-changes-the-color-of-pics\n","#https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\n","#https://www.geeksforgeeks.org/python-convert-a-string-representation-of-list-into-list/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44188,"status":"ok","timestamp":1637600140057,"user":{"displayName":"LEE WAN NING _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11525349523156579986"},"user_tz":-480},"id":"V6q6HkoZtFnl","outputId":"4cef7f8d-5b8c-4efa-8055-478dffbfabc6"},"outputs":[],"source":["#Building a classification model to determine if an MRT image would be deemed as Beautiful/Safe/Welcome or not. \n","\n","#reading data (from excel) using pandas\n","img_metrics=[\"Beautiful\",\"Safe\",\"Welcome\"]\n","for i in img_metrics:\n","  X=pd.read_excel(\"/content/drive/MyDrive/imgBOVW.xlsx\",sheet_name=\"{}\".format(i), usecols=\"B:DL\",header=[0])\n","  Y=pd.read_excel(\"/content/drive/MyDrive/imgBOVW.xlsx\",sheet_name=\"{}\".format(i), usecols=\"A\",header=[0])\n","\n","#setting classification rules\n","  Y.loc[Y[i] <= 3, i] = 0\n","  Y.loc[Y[i] > 3, i] = 1\n","\n","\n","  #scaling of data\n","  stdslr=StandardScaler()\n","  X=stdslr.fit_transform(X)\n","\n","  #SVM, a supervised learning classification algorithm is used \n","  clf=LinearSVC(max_iter=80000,random_state=1)\n","  skf = StratifiedKFold(n_splits=5)\n","  eva_dict={\"accuracy score\":[],\"precision score\":[],\"recall score\":[],\"F1 score\":[]}\n","  for train_index, test_index in skf.split(X, Y):\n","    X_train,X_test=X[train_index],X[test_index]\n","    Y_train,Y_test=Y.iloc[train_index],Y.iloc[test_index]\n","    clf.fit(X_train,Y_train)\n","    Y_pred=clf.predict(X_test)\n","\n","    #evaluation of classfication model\n","    eva_dict[\"accuracy score\"].append(metrics.accuracy_score(Y_test, Y_pred))\n","    eva_dict[\"precision score\"].append(metrics.precision_score(Y_test, Y_pred))\n","    eva_dict[\"recall score\"].append(metrics.recall_score(Y_test, Y_pred))\n","    eva_dict[\"F1 score\"].append(metrics.f1_score(Y_test, Y_pred))\n","  #export the evaluation results  to excel sheet (4 sets for each image quality)\n","  df2=pd.DataFrame.from_dict(eva_dict,orient=\"columns\")\n","  with pd.ExcelWriter('/content/drive/MyDrive/imgBOVW.xlsx',mode='a') as writer:  \n","    df2.to_excel(writer, sheet_name='{}_modeleval'.format(i))\n","  #calculating the overall  mean and standard deviation of the evaluation result, for each image quality\n","  print(\"mean:\",df2.mean())\n","  print(\"std:\",df2.std())\n","\n","  #References\n","#https://stackoverflow.com/questions/54762004/converting-continuous-column-to-binary-in-python-based-on-some-condition\n","#https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n","#://stackoverflow.com/questions/64492004/stratified-kfold-vs-train-test-split-what-training-data-is-used\n","#https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/\n","#https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n","#https://www.analyseup.com/python-machine-learning/stratified-kfold.html"]}],"metadata":{"colab":{"name":"Image Analysis (bag of visual words) .ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"}}},"nbformat":4,"nbformat_minor":0}
